{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_prediction_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n851RlE0q62q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84a706b-3a4b-4599-b978-784e7fb02b31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import deque\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7lzN8Kpsu7U"
      },
      "source": [
        "SEQ_LEN = 60 \n",
        "FUTURE_PERIOD_PREDICT = 3  \n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ttcxDDMszjh"
      },
      "source": [
        "def classify(current, future):\n",
        "    if float(future) > float(current):  \n",
        "        return 1\n",
        "    else: \n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_7KOtbts26H"
      },
      "source": [
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)\n",
        "\n",
        "    for col in df.columns:  \n",
        "        if col != \"target\":  \n",
        "            df[col] = df[col].pct_change()  \n",
        "            df.dropna(inplace=True)  \n",
        "            df[col] = preprocessing.scale(df[col].values)  \n",
        "\n",
        "    df.dropna(inplace=True) \n",
        "\n",
        "\n",
        "    sequential_data = [] \n",
        "    prev_days = deque(maxlen=SEQ_LEN) \n",
        "\n",
        "    for i in df.values:  \n",
        "        prev_days.append([n for n in i[:-1]]) \n",
        "        if len(prev_days) == SEQ_LEN:  \n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  \n",
        "\n",
        "    random.shuffle(sequential_data)\n",
        "\n",
        "    buys = [] \n",
        "    sells = [] \n",
        "\n",
        "    for seq, target in sequential_data:  \n",
        "        if target == 0:  \n",
        "            sells.append([seq, target])  \n",
        "        elif target == 1:  \n",
        "            buys.append([seq, target])  \n",
        "\n",
        "    random.shuffle(buys)  \n",
        "    random.shuffle(sells)  \n",
        "\n",
        "    lower = min(len(buys), len(sells))  \n",
        "\n",
        "    buys = buys[:lower]  \n",
        "    sells = sells[:lower] \n",
        "\n",
        "    sequential_data = buys+sells  \n",
        "    random.shuffle(sequential_data)  \n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data: \n",
        "        X.append(seq)  \n",
        "        y.append(target)  \n",
        "\n",
        "    return np.array(X), y  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHqiW3tUtJG_",
        "outputId": "b2a04d3b-e13d-4104-f2ce-86604f152fba"
      },
      "source": [
        "main_df = pd.DataFrame() \n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  \n",
        "for ratio in ratios:  \n",
        "\n",
        "    ratio = ratio.split('.csv')[0]  \n",
        "    dataset = f'/content/gdrive/My Drive/crypto_data/{ratio}.csv' \n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume']) \n",
        "\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True) \n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  \n",
        "\n",
        "    if len(main_df)==0: \n",
        "        main_df = df \n",
        "    else:  \n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True) \n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
        "train_x = np.asarray(train_x)\n",
        "train_y = np.asarray(train_y)\n",
        "validation_x = np.asarray(validation_x)\n",
        "validation_y = np.asarray(validation_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 77922 validation: 3860\n",
            "Dont buys: 38961, buys: 38961\n",
            "VALIDATION Dont buys: 1930, buys: 1930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSySkQdAtXAI"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, activation='relu', input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDTE5nceY01Z",
        "outputId": "d224bb72-b967-4b67-9056-4bbd87090dff"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y)\n",
        ")\n",
        "\n",
        "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "model.save(\"models/{}\".format(NAME))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1218/1218 [==============================] - 507s 413ms/step - loss: 0.7170 - accuracy: 0.5131 - val_loss: 0.9249 - val_accuracy: 0.4982\n",
            "Epoch 2/10\n",
            "1218/1218 [==============================] - 500s 410ms/step - loss: 0.6921 - accuracy: 0.5199 - val_loss: 0.6938 - val_accuracy: 0.5202\n",
            "Epoch 3/10\n",
            "1218/1218 [==============================] - 500s 410ms/step - loss: 0.6911 - accuracy: 0.5245 - val_loss: 0.6936 - val_accuracy: 0.5080\n",
            "Epoch 4/10\n",
            "1218/1218 [==============================] - 497s 408ms/step - loss: 0.6910 - accuracy: 0.5249 - val_loss: 0.6940 - val_accuracy: 0.5054\n",
            "Epoch 5/10\n",
            "1218/1218 [==============================] - 509s 418ms/step - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6943 - val_accuracy: 0.5028\n",
            "Epoch 6/10\n",
            "1218/1218 [==============================] - 500s 410ms/step - loss: 0.6903 - accuracy: 0.5282 - val_loss: 0.6926 - val_accuracy: 0.5225\n",
            "Epoch 7/10\n",
            "1218/1218 [==============================] - 494s 406ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6929 - val_accuracy: 0.5080\n",
            "Epoch 8/10\n",
            "1218/1218 [==============================] - 493s 405ms/step - loss: 0.6931 - accuracy: 0.5053 - val_loss: 0.6930 - val_accuracy: 0.4997\n",
            "Epoch 9/10\n",
            "1218/1218 [==============================] - 499s 410ms/step - loss: 0.6929 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5075\n",
            "Epoch 10/10\n",
            "1218/1218 [==============================] - 495s 406ms/step - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6931 - val_accuracy: 0.4997\n",
            "Test loss: 0.6930762529373169\n",
            "Test accuracy: 0.49974092841148376\n",
            "INFO:tensorflow:Assets written to: models/60-SEQ-3-PRED-1638872178/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1eec67aed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1eec7f1e90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1eec823750> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN6zNNMCZVZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}